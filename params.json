{
  "name": "Hadoop How To",
  "tagline": "Short, targeted, guides to all things Hadoop",
  "body": "### See the essence of the Spark Streaming API.\r\n\r\n[Spark Streaming](sparkStreaming/sparkStreaming.html)\r\n\r\n### Using the Scala sbt build tool with Spark code.\r\n\r\n[sbt build for Scala/Spark](sbt/sbt.html)\r\n\r\n### How to unit test Scala Spark code.\r\n\r\n[Spark Unit Testing](sparkUnitTest/sparkUnitTest.html)\r\n\r\n### Here is how to setup and run SparkR on a cluster.\r\n\r\n[SparkR](SparkR/SparkR.html)\r\n\r\n### This shows how to add Scala functions that look like they are part of the Spark API.\r\n\r\n[Extending Spark](extendingSpark/extendingSpark.html)  \r\n\r\n### Joins in Hive using both reduce-side and bucket map-side approaches.\r\n\r\n[Hive Joins](hiveJoins/hiveJoins.html)  \r\n\r\n### Here is the classic wordcount example, on Spark YARN, using the three language APIs.\r\n\r\n[Scala Wordcount](sparkScala/sparkScala.html)  \r\n[Java Wordcount](sparkJava/sparkJava.html)  \r\n[Python Wordcount, coming soon]\r\n\r\n### Here is the classic wordcount example, using the new Hadoop API.\r\n\r\n[Wordcount]\r\n(wordcount/wordcount.html)\r\n\r\n### Here are two methods for chaining and managing multiple MapReduce Jobs.\r\n\r\n[Chaining and Managing Multiple MapReduce Jobs with two drivers](multipleJobsShell/multipleJobsShell.html)  \r\n[Chaining and Managing Multiple MapReduce Jobs with one driver](multipleJobsSingle/multipleJobsSingle.html)\r\n\r\n### In order to inspect work node logs in a Hadoop cluster that is behind a firewall with only SSH access, a browser must be setup for tunneling.\r\n\r\n[Using a browser to tunnel into a Hadoop cluster to inspect worker node logs](browserTunneling/browserTunneling.html)\r\n\r\n### Hadoop MapReduce can write key/value output to HDFS in a variety of formats. Here is how to display them.\r\n\r\n[Display Your MapReduce Output](displayOutput/displayOutput.html)\r\n\r\n### MRUnit supports two different input/output methods, add and with.  Here is the difference.\r\n\r\n[MRUnit: with vs. add](withMRUnit/withMRUnit.html)\r\n\r\n### This four-part series shows how to pass multiple values from a mapper to a reducer, and from the reducer to output.\r\n\r\n[Passing Multiple Values in MapReduce Part 1: Strings](stringMultipleValues/stringMultipleValues.html)  \r\n[Passing Multiple Values in MapReduce Part 2: Custom Writables](writableMultipleValues/writableMultipleValues.html)  \r\n[Passing Multiple Values in MapReduce Part 3: Maps](mapMultipleValues/mapMultipleValues.html)  \r\n[Passing Multiple Values in MapReduce Part 4: AVRO](AVROMultipleValues/AVROMultipleValues.html)\r\n\r\n### Using the software lifecycle and build tool Maven, you can configure Eclipse for Hadoop development in minutes.\r\n\r\n[Setup Eclipse for Hadoop Development Using Maven (Linux/Mac version)](hadoopMaven/hadoopMaven.html)\r\n\r\n[Setup Eclipse for Hadoop Development Using Maven (Windows version)](hadoopMavenWindows/hadoopMavenWindows.html)\r\n\r\n----\r\n\r\n### Questions about this material?  \r\n[Hadoop Concepts Forum](http://community.cloudera.com/t5/Apache-Hadoop-Concepts-and/bd-p/ApacheHadoopConcepts)\r\n\r\n----\r\n\r\n### These guides are brought to you by\r\n\r\nCenter of Excellence for Big Data (CoE4BD)  \r\nGraduate Programs in Software  \r\nUniversity of St. Thomas  \r\nSt. Paul, Minnesota\r\n\r\nhttp://www.stthomas.edu/CoE4BD  \r\nCoE4BD@stthomas.edu  \r\n[@CoE4BD](http://www.twitter.com/coe4bd)\r\n\r\nIn collaboration with [Cloudera](http://www.cloudera.com) and their Academic Partnership program \r\n\r\nAlso see our [Technical Reports](http://coe4bd.github.io/TechnicalReports)",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}