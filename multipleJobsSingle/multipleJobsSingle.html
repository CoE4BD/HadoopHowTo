<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<style>
h1,
h2,
h3,
h4,
h5,
h6,
p,
blockquote {
    margin: 0;
    padding: 0;
}
body {
    font-family: "Helvetica Neue", Helvetica, "Hiragino Sans GB", Arial, sans-serif;
    font-size: 13px;
    line-height: 18px;
    color: #737373;
    background-color: white;
    margin: 10px 13px 10px 13px;
}
table {
	margin: 10px 0 15px 0;
	border-collapse: collapse;
}
td,th {	
	border: 1px solid #ddd;
	padding: 3px 10px;
}
th {
	padding: 5px 10px;	
}

a {
    color: #0069d6;
}
a:hover {
    color: #0050a3;
    text-decoration: none;
}
a img {
    border: none;
}
p {
    margin-bottom: 9px;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    color: #404040;
    line-height: 36px;
}
h1 {
    margin-bottom: 18px;
    font-size: 30px;
}
h2 {
    font-size: 24px;
}
h3 {
    font-size: 18px;
}
h4 {
    font-size: 16px;
}
h5 {
    font-size: 14px;
}
h6 {
    font-size: 13px;
}
hr {
    margin: 0 0 19px;
    border: 0;
    border-bottom: 1px solid #ccc;
}
blockquote {
    padding: 13px 13px 21px 15px;
    margin-bottom: 18px;
    font-family:georgia,serif;
    font-style: italic;
}
blockquote:before {
    content:"\201C";
    font-size:40px;
    margin-left:-10px;
    font-family:georgia,serif;
    color:#eee;
}
blockquote p {
    font-size: 14px;
    font-weight: 300;
    line-height: 18px;
    margin-bottom: 0;
    font-style: italic;
}
code, pre {
    font-family: Monaco, Andale Mono, Courier New, monospace;
}
code {
    background-color: #fee9cc;
    color: rgba(0, 0, 0, 0.75);
    padding: 1px 3px;
    font-size: 12px;
    -webkit-border-radius: 3px;
    -moz-border-radius: 3px;
    border-radius: 3px;
}
pre {
    display: block;
    padding: 14px;
    margin: 0 0 18px;
    line-height: 16px;
    font-size: 11px;
    border: 1px solid #d9d9d9;
    white-space: pre-wrap;
    word-wrap: break-word;
}
pre code {
    background-color: #fff;
    color:#737373;
    font-size: 11px;
    padding: 0;
}
sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:10px auto;
    }
}
@media print {
	body,code,pre code,h1,h2,h3,h4,h5,h6 {
		color: black;
	}
	table, pre {
		page-break-inside: avoid;
	}
}
</style>
<title>Chaining and Managing Multiple MapReduce Jobs with One Driver</title>

</head>
<body>
<h2>Chaining and Managing Multiple MapReduce Jobs with One Driver</h2>

<p>Lawrence Kyei &amp; Brad Rubin<br/>
3/22/2016</p>

<p>In this How-To, we look at chaining two MapReduce jobs together to solve a simple WordCount problem with one driver for both jobs.</p>

<h3>The Two MapReduce Jobs</h3>

<p>This is a simple WordCount problem which uses two MapReduce jobs. The first job is a standard WordCount program that outputs the word as the key and the count of the word as the value in the directory <strong>output/temp</strong>. The second MapReduce job swaps key and value so that we get words sorted in descending order by frequency. This spits the results in the directory <strong>output2/final</strong>. Both jobs are executed with a single driver.</p>

<p>The jar file <strong>SEIS736-1.0.jar</strong> consist of;</p>

<ul>
<li>2 Mappers</li>
<li>2 Reducers</li>
<li>1 Driver</li>
<li>IntComparator Class</li>
</ul>


<p><strong>WordMapper.java</strong></p>

<pre><code>package stubs;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {

  IntWritable intWritable = new IntWritable(1);
  Text text = new Text();

  @Override
  public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

    String line = value.toString();
    for (String word : line.split("\\W+")) {
        if (word.length() &gt; 0) {
            text.set(word);
            context.write(text, intWritable);
         }
     }
  }
} 
</code></pre>

<p><strong>SumReducer.java</strong></p>

<pre><code>package stubs;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class SumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {

IntWritable intWritable = new IntWritable();

@Override
public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {

    int wordCount = 0;
    for (IntWritable value : values) {
        wordCount += value.get();
    }
    intWritable.set(wordCount);
    context.write(key, intWritable);
  }
}
</code></pre>

<p><strong>WordMapper2.java</strong></p>

<pre><code>package stubs;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordMapper2 extends Mapper&lt; Text, Text, IntWritable, Text&gt; {

  IntWritable frequency = new IntWritable();

  @Override
  public void map(Text key, Text value, Context context)
    throws IOException, InterruptedException {

    int newVal = Integer.parseInt(value.toString());
    frequency.set(newVal);
    context.write(frequency, key);
  }
}
</code></pre>

<p><strong>SumReducer2.java</strong></p>

<pre><code>package stubs;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class SumReducer2 extends Reducer&lt;IntWritable, Text, IntWritable, Text&gt; {

  Text word = new Text();

  @Override
  public void reduce(IntWritable key, Iterable&lt;Text&gt; values, Context context)
        throws IOException, InterruptedException {

    for (Text value : values) {
        word.set = value;
        context.write(key, word);
    }
  }
}
</code></pre>

<p><strong>WordCombined.java</strong></p>

<pre><code>package stubs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;   
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;
import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;

public class WordCombined extends Configured implements Tool {

 public int run(String[] args) throws Exception {

    JobControl jobControl = new JobControl("jobChain"); 
    Configuration conf1 = getConf();

    Job job1 = Job.getInstance(conf1);  
    job1.setJarByClass(WordCombined.class);
    job1.setJobName("Word Combined");

    FileInputFormat.setInputPaths(job1, new Path(args[0]));
    FileOutputFormat.setOutputPath(job1, new Path(args[1] + "/temp"));

    job1.setMapperClass(WordMapper.class);
    job1.setReducerClass(SumReducer.class);
    job1.setCombinerClass(SumReducer.class);

    job1.setOutputKeyClass(Text.class);
    job1.setOutputValueClass(IntWritable.class);

    ControlledJob controlledJob1 = new ControlledJob(conf1);
    controlledJob1.setJob(job1);

    jobControl.addJob(controlledJob1);
    Configuration conf2 = getConf();

    Job job2 = Job.getInstance(conf2);
    job2.setJarByClass(WordCombined.class);
    job2.setJobName("Word Invert");

    FileInputFormat.setInputPaths(job2, new Path(args[1] + "/temp"));
    FileOutputFormat.setOutputPath(job2, new Path(args[1] + "/final"));

    job2.setMapperClass(WordMapper2.class);
    job2.setReducerClass(SumReducer2.class);
    job2.setCombinerClass(SumReducer2.class);

    job2.setOutputKeyClass(IntWritable.class);
    job2.setOutputValueClass(Text.class);
    job2.setInputFormatClass(KeyValueTextInputFormat.class);

    job2.setSortComparatorClass(IntComparator.class);
    ControlledJob controlledJob2 = new ControlledJob(conf2);
    controlledJob2.setJob(job2);

    // make job2 dependent on job1
    controlledJob2.addDependingJob(controlledJob1); 
    // add the job to the job control
    jobControl.addJob(controlledJob2);
    Thread jobControlThread = new Thread(jobControl);
    jobControlThread.start();

while (!jobControl.allFinished()) {
    System.out.println("Jobs in waiting state: " + jobControl.getWaitingJobList().size());  
    System.out.println("Jobs in ready state: " + jobControl.getReadyJobsList().size());
    System.out.println("Jobs in running state: " + jobControl.getRunningJobList().size());
    System.out.println("Jobs in success state: " + jobControl.getSuccessfulJobList().size());
    System.out.println("Jobs in failed state: " + jobControl.getFailedJobList().size());
try {
    Thread.sleep(5000);
    } catch (Exception e) {

    }

  } 
   System.exit(0);  
   return (job1.waitForCompletion(true) ? 0 : 1);   
  } 
  public static void main(String[] args) throws Exception { 
  int exitCode = ToolRunner.run(new WordCombined(), args);  
  System.exit(exitCode);
  }
}
</code></pre>

<p><strong>IntComparator.java</strong></p>

<pre><code>package stubs;
import java.nio.ByteBuffer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.WritableComparator;

public class IntComparator extends WritableComparator {

  public IntComparator() {
    super(IntWritable.class);
  }

  @Override
  public int compare(byte[] b1, int s1, int l1, byte[] b2,
        int s2, int l2) {
    Integer v1 = ByteBuffer.wrap(b1, s1, l1).getInt();
    Integer v2 = ByteBuffer.wrap(b2, s2, l2).getInt();
    return v1.compareTo(v2) * (-1);
  }
}
</code></pre>

<h3>Running Code</h3>

<pre><code>hadoop fs -rm -r output
hadoop jar SEIS736-1.0.jar stubs.WordCount2 -D mapred.reduce.tasks=1 shakespeare output
</code></pre>

<h3>Results</h3>

<p>Let us take a look at the top 10 results in the part-r-00000 file in the directory  <strong>output/final</strong></p>

<pre><code>$ hadoop fs -cat output/final/part-r-00000 | head -10
25578   the
23027   I
19654   and
17462   to
16444   of
13524   a
12697   you
11296   my
10699   in
8857    is
</code></pre>
</body>
</html>